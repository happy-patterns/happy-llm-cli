id: 6
name: "Phase 5: MVP Testing & Documentation (Granular)"
description: >
  Verify the functionality of the MVP through testing and create essential
  user documentation. Granular breakdown.
model: null
prompt_template: null

subtasks:
  # 6.1 Unit Tests
  - id: 6.1.1
    name: "Setup Test Directory Structure"
    details: |
      - Create the `tests/` directory in the project root.
      - Create `tests/__init__.py`.
      - Create mirroring subdirectories if desired: `tests/utils/`, `tests/providers/`.
      - Create empty test files: `tests/utils/test_config.py`, `tests/utils/test_rate_limit.py`, `tests/providers/test_openai_provider.py`.
    ai_assisted: false
    dependencies:
      - 1.2.1
    outputs:
      - "`tests/` directory and basic test file structure created."

  - id: 6.1.2
    name: "Ensure Test Dependencies"
    details: |
      - Verify testing framework (`pytest`) and mocking libraries (`pytest-mock`, `requests-mock` - optional) are listed in `requirements-dev.txt` or a `[project.optional-dependencies]` group in `pyproject.toml`.
      - If not, add them.
      - Ensure these dependencies are installed in the development environment (`pip install -r requirements-dev.txt` or `pip install .[dev]`).
    ai_assisted: false
    dependencies:
      - 6.1.1
    outputs:
      - "Test dependencies (`pytest`, mocks) confirmed/added."
      - "Development environment updated with test dependencies."

  - id: 6.1.3
    name: "Generate Unit Tests for Config Util"
    details: |
      Use AI to generate `pytest` functions for `tests/utils/test_config.py` to test `load_api_key` (success and failure cases) using mocks (`@patch` or `mocker` fixture).
    ai_assisted: true
    model: "gpt-4.1-mini"
    prompt_template: |
      Goal: Generate ONLY the Python code snippet for Pytest unit tests for the `load_api_key` function.
      Context:
        - Target file: `tests/utils/test_config.py`.
        - Function to test: `load_api_key` from `happy_llm_cli.utils.config`.
        - Needs mocking for `os.getenv` and `dotenv.load_dotenv`. Use `mocker` fixture from `pytest-mock`.
        - Test Cases:
            1. Success: `os.getenv` returns a valid key.
            2. Failure: `os.getenv` returns `None`. (Expect `ValueError`).
            3. Failure: `os.getenv` returns an empty string. (Expect `ValueError`).
      Instructions:
        - Import `pytest`, `load_api_key`.
        - Create test function `test_load_api_key_success(mocker)`: Mock `load_dotenv` and `os.getenv` to return 'test-key'. Call `load_api_key` and assert result. Assert mocks were called.
        - Create test function `test_load_api_key_missing(mocker)`: Mock `load_dotenv` and `os.getenv` to return `None`. Use `pytest.raises(ValueError)` to assert exception.
        - Create test function `test_load_api_key_empty(mocker)`: Mock `load_dotenv` and `os.getenv` to return "". Use `pytest.raises(ValueError)`.
      Output Format: Python code snippet containing the test functions and necessary imports.
    dependencies:
      - 4.1.4
      - 6.1.2
    outputs:
      - "Python code snippet for `test_config.py` generated."

  - id: 6.1.4
    name: "Review Unit Tests for Config Util"
    details: |
      Manually review the generated test code for `test_config.py`:
        - Check imports, use of `mocker` or `@patch`.
        - Verify mocks are configured correctly for each case (return value of `getenv`).
        - Confirm assertions check return value (success) or raised exception (`ValueError` for failures).
        - Ensure tests cover specified cases.
    ai_assisted: false
    dependencies:
      - 6.1.3
    outputs:
      - "Code snippet for `test_config.py` reviewed and approved/corrected."

  - id: 6.1.5
    name: "Write Unit Tests for Config Util"
    details: |
      - Open `tests/utils/test_config.py`.
      - Write the reviewed test code into the file.
    ai_assisted: false
    dependencies:
      - 6.1.1
      - 6.1.4
    outputs:
      - "`tests/utils/test_config.py` populated with tests."

  - id: 6.1.6
    name: "Generate Unit Tests for Rate Limit Util"
    details: |
      Use AI to generate `pytest` functions for `tests/utils/test_rate_limit.py` to test `retry_api_call` (success on first try, success after retries, failure after max retries, immediate failure on other exception) using mocks for `func`, `time.sleep`, `RateLimitError`.
    ai_assisted: true
    model: "o4-mini"
    prompt_template: |
      Goal: Generate ONLY the Python code snippet for Pytest unit tests for the `retry_api_call` function.
      Context:
        - Target file: `tests/utils/test_rate_limit.py`.
        - Function to test: `retry_api_call` from `happy_llm_cli.utils.rate_limit`.
        - Needs mocking for the callable `func`, `time.sleep`, and `RateLimitError` from `happy_llm_cli.providers.exceptions`. Use `mocker` fixture.
        - Test Cases:
            1. Success on first try.
            2. Success after 1 retry (func raises `RateLimitError` once, then succeeds).
            3. Failure after max retries (func always raises `RateLimitError`). Expect `RateLimitError`.
            4. Immediate failure (func raises other `Exception` like `ValueError`). Expect `ValueError`.
      Instructions:
        - Import `pytest`, `time`, `retry_api_call`, `RateLimitError`, `ValueError`.
        - Mock `time.sleep` using `mocker`.
        - Write test functions for each case, asserting results and mock call counts.
      Output Format: Python code snippet containing the test functions and necessary imports.
    dependencies:
      - 4.2.3
      - 2.4.3
      - 6.1.2
    outputs:
      - "Python code snippet for `test_rate_limit.py` generated."

  - id: 6.1.7
    name: "Review Unit Tests for Rate Limit Util"
    details: |
      Manually review the generated test code for `test_rate_limit.py`:
        - Check imports, use of `mocker`.
        - Verify mock setups for `func` (side effects and return values).
        - Confirm assertions check return values, raised exceptions, and mock call counts.
    ai_assisted: false
    dependencies:
      - 6.1.6
    outputs:
      - "Code snippet for `test_rate_limit.py` reviewed and approved/corrected."

  - id: 6.1.8
    name: "Write Unit Tests for Rate Limit Util"
    details: |
      - Open `tests/utils/test_rate_limit.py`.
      - Write the reviewed test code into the file.
    ai_assisted: false
    dependencies:
      - 6.1.1
      - 6.1.7
    outputs:
      - "`tests/utils/test_rate_limit.py` populated with tests."

  - id: 6.1.9
    name: "Generate Unit Tests for OpenAI Provider"
    details: |
      Use AI to generate `pytest` functions for `tests/providers/test_openai_provider.py`. Test `__init__` and `complete_chat` using mocks for `requests.post`, covering success (200), auth error (401), rate limit error (429), server error (500), network error, and JSON decode error.
    ai_assisted: true
    model: "o4-mini"
    prompt_template: |
      Goal: Generate ONLY the Python code snippet for Pytest unit tests for the `OpenAIProvider`.
      Context:
        - Target file: `tests/providers/test_openai_provider.py`.
        - Class: `OpenAIProvider` from `happy_llm_cli.providers.openai_provider`.
        - Needs mocking for `requests.post`. Configure response `status_code` and `json()`. 
        - Exceptions: `AuthenticationError`, `RateLimitError`, `ProviderResponseError`, `requests.exceptions.RequestException`, `json.JSONDecodeError`.
        - Test Cases for `complete_chat`:
            1. Success (200).
            2. Auth Error (401).
            3. Rate Limit Error (429).
            4. Server Error (500).
            5. Network Error.
            6. JSON Decode Error.
        - Test Cases for `__init__`:
            1. Valid API Key.
            2. Missing/Empty API Key (ValueError).
            3. Default model when None.
      Instructions:
        - Import necessary classes and exceptions.
        - Mock `requests.post` using `mocker.patch`.
        - Write tests using `pytest.raises` for error scenarios and assertions for success.
      Output Format: Python code snippet containing the test functions and necessary imports.
    dependencies:
      - 3.4.3
      - 6.1.2
    outputs:
      - "Python code snippet for `test_openai_provider.py` generated."

  - id: 6.1.10
    name: "Review Unit Tests for OpenAI Provider"
    details: |
      Manually review the generated test code for `test_openai_provider.py`:
        - Check imports and fixtures.
        - Verify `requests.post` mock setups for each scenario.
        - Confirm assertions for returned `ChatResponse` or raised exceptions.
    ai_assisted: false
    dependencies:
      - 6.1.9
    outputs:
      - "Code snippet for `test_openai_provider.py` reviewed and approved/corrected."

  - id: 6.1.11
    name: "Write Unit Tests for OpenAI Provider"
    details: |
      - Open `tests/providers/test_openai_provider.py`.
      - Write the reviewed test code into the file.
    ai_assisted: false
    dependencies:
      - 6.1.1
      - 6.1.10
    outputs:
      - "`tests/providers/test_openai_provider.py` populated with tests."

  - id: 6.1.12
    name: "Commit Unit Tests"
    details: |
      - Stage the `tests/` directory and any changes to dependency files.
      - Run `git add tests/ requirements-dev.txt pyproject.toml`.
      - Run `git commit -m \"test: Add unit tests for utilities and OpenAI provider\"`.
    ai_assisted: false
    dependencies:
      - 6.1.5
      - 6.1.8
      - 6.1.11
      - 6.1.2
      - 1.3.6
    outputs:
      - "Unit tests committed to Git."

  - id: 6.1.13
    name: "Run Unit Tests"
    details: |
      - Execute the test suite from the project root: `pytest`.
      - Verify all tests pass. Debug any failures.
    ai_assisted: false
    dependencies:
      - 6.1.12
    outputs:
      - "Pytest execution report showing passing tests."

  # 6.2 Manual Testing
  - id: 6.2.1
    name: "Prepare Environment for Manual Testing"
    details: |
      - Ensure the development container is running.
      - Install the package in editable mode: `pip install -e .`.
      - Verify the `.env` file contains a valid `OPENAI_API_KEY`.
    ai_assisted: false
    dependencies:
      - 5.4.4
      - 1.5.1
      - 1.1.4
    outputs:
      - "Package installed editable in container."
      - "API key confirmed present for testing."

  - id: 6.2.2```
