id: 3
name: "Phase 2: Implement OpenAI Provider Adapter (Granular)"
description: >
  Create the concrete implementation for OpenAI, including API interaction,
  response parsing, and basic error handling, adhering to the `AbstractProviderAdapter`
  interface. Granular breakdown.
model: null
prompt_template: null

subtasks:
  - id: 3.1.1
    name: "Generate OpenAIProvider Skeleton Code Snippet"
    details: |
      Use AI to generate the basic class definition, imports, constants,
      `__init__` method (with API key validation and model default logic),
      and placeholder `complete_chat` method signature.
    ai_assisted: true
    model: "gpt-4.1-mini"
    prompt_template: |
      Goal: Generate ONLY the Python code snippet for the initial skeleton of the `OpenAIProvider` class.
      Context:
        - Target file: `happy_llm_cli/providers/openai_provider.py`.
        - Inherits from `AbstractProviderAdapter` in `.base`.
        - Needs `__init__(self, api_key: str, model: Optional[str] = None)` storing validated key and model (defaulting to 'gpt-4o-mini').
        - Needs placeholder `complete_chat(self, request: ChatRequest) -> ChatResponse`.
        - Requires imports: `.base` classes, `typing`, `requests`.
        - Needs constants: `OPENAI_API_URL`, `DEFAULT_OPENAI_MODEL`.
      Instructions:
        - Import `AbstractProviderAdapter`, `ChatRequest`, `ChatResponse` from `.base`.
        - Import `Optional`, `Dict`, `Any`, `List` from `typing`.
        - Import `requests`.
        - Define `OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"`.
        - Define `DEFAULT_OPENAI_MODEL = "gpt-4o-mini"`.
        - Define `class OpenAIProvider(AbstractProviderAdapter):`.
        - Implement `__init__` method: store `api_key` (e.g., `self._api_key`) after validating non-empty; store `model` using fallback.
        - Implement `complete_chat` signature with `pass` or `raise NotImplementedError`.
        - DO NOT include implementation logic.
      Output Format: Python code snippet for skeleton, imports, and constants.
    dependencies:
      - 2.1.3
      - 2.2.3
    outputs:
      - "Python code snippet for `OpenAIProvider` skeleton generated."

  - id: 3.1.2
    name: "Review OpenAIProvider Skeleton Code Snippet"
    details: |
      Manually review the generated skeleton:
        - Imports, constants, inheritance, method signatures, no extras.
    ai_assisted: false
    dependencies:
      - 3.1.1
    outputs:
      - "`OpenAIProvider` skeleton reviewed and corrected."

  - id: 3.1.3
    name: "Write OpenAIProvider Skeleton to File"
    details: |
      Create `happy_llm_cli/providers/openai_provider.py` and insert reviewed skeleton code.
    ai_assisted: false
    dependencies:
      - 3.1.2
      - 1.2.2
    outputs:
      - "`openai_provider.py` created with skeleton."

  - id: 3.1.4
    name: "Ensure `requests` Dependency"
    details: |
      Verify that `requests` is listed in the `[project.dependencies]` section of `pyproject.toml`.
      If missing, add it, then reinstall dependencies (e.g., `pip install -e .` or rebuild the container).
    ai_assisted: false
    dependencies:
      - 3.1.3
    outputs:
      - "`requests` confirmed in dependencies."

  - id: 3.1.5
    name: "Commit OpenAIProvider Skeleton"
    details: |
      Stage and commit skeleton files:
      Run:
        git add happy_llm_cli/providers/openai_provider.py pyproject.toml
        git commit -m "feat: Add OpenAIProvider skeleton"
    ai_assisted: false
    dependencies:
      - 3.1.3
      - 3.1.4
      - 1.3.6
    outputs:
      - "`OpenAIProvider` skeleton committed."

  - id: 3.2.1
    name: "Generate Request Preparation Code Snippet"
    details: |
      Use AI to generate code for headers and payload prep inside `complete_chat`.
    ai_assisted: true
    model: "o4-mini"
    prompt_template: |
      Goal: Generate ONLY the Python code snippet for request header and payload preparation.
      Context: Inside `complete_chat`, use `self._api_key`, `self.model`, and `request.messages`.
      Instructions:
        - Build `headers` and `data` dicts as required by OpenAI API.
      Output: Code snippet for headers and payload only.
    dependencies:
      - 3.1.3
    outputs:
      - "Request preparation snippet generated."

  - id: 3.2.2
    name: "Review Request Preparation Code Snippet"
    details: |
      Verify header/payload logic, correct attribute usage, no hardcoded values.
    ai_assisted: false
    dependencies:
      - 3.2.1
    outputs:
      - "Request preparation snippet reviewed."

  - id: 3.2.3
    name: "Integrate Request Preparation into complete_chat"
    details: |
      Insert reviewed request prep code into `complete_chat` replacing placeholder.
    ai_assisted: false
    dependencies:
      - 3.1.3
      - 3.2.2
    outputs:
      - "Request prep logic integrated."

  - id: 3.3.1
    name: "Generate API Call Code Snippet"
    details: |
      Use AI to generate `requests.post` call and connection error handling snippet.
    ai_assisted: true
    model: "o4-mini"
    prompt_template: |
      Goal: Generate snippet for `requests.post` and `except RequestException` only.
      Context: After headers/data prep, use `OPENAI_API_URL`, `headers`, `data`.
      Instructions: Provide try/except as per spec.
    dependencies:
      - 3.2.3
    outputs:
      - "API call snippet generated."

  - id: 3.3.2
    name: "Review API Call Code Snippet"
    details: |
      Verify `requests.post` usage, exception handling, no status parsing.
    ai_assisted: false
    dependencies:
      - 3.3.1
    outputs:
      - "API call snippet reviewed."

  - id: 3.3.3
    name: "Integrate API Call into complete_chat"
    details: |
      Insert reviewed API call snippet into `complete_chat`.
    ai_assisted: false
    dependencies:
      - 3.2.3
      - 3.3.2
    outputs:
      - "API call logic integrated."

  - id: 3.4.1
    name: "Generate Response Handling Code Snippet"
    details: |
      Use AI to generate response parsing and error handling snippet.
    ai_assisted: true
    model: "o4-mini"
    prompt_template: |
      Goal: Generate snippet for response status checks, JSON parsing, error handling.
      Context: After `response = requests.post(...)`.
      Instructions: Follow spec for HTTP codes, JSON errors, custom exceptions.
    dependencies:
      - 3.3.3
    outputs:
      - "Response handling snippet generated."

  - id: 3.4.2
    name: "Review Response Handling Code Snippet"
    details: |
      Verify status checks, exception strategy, JSON error handling.
    ai_assisted: false
    dependencies:
      - 3.4.1
    outputs:
      - "Response handling snippet reviewed."

  - id: 3.4.3
    name: "Integrate Response Handling into complete_chat"
    details: |
      Insert reviewed response handling snippet into `complete_chat`.
    ai_assisted: false
    dependencies:
      - 3.3.3
      - 3.4.2
    outputs:
      - "Response handling logic integrated."

  - id: 3.4.4
    name: "Commit complete_chat Implementation"
    details: |
      Stage and commit `openai_provider.py` updates:
      Run:
        git add happy_llm_cli/providers/openai_provider.py
        git commit -m "feat: Implement complete_chat in OpenAIProvider"
    ai_assisted: false
    dependencies:
      - 3.4.3
      - 3.1.5
    outputs:
      - "`complete_chat` implementation committed."

  - id: 3.5.1
    name: "[Optional] Generate Logging Code Snippets"
    details: |
      If proceeding: Use AI to generate logging statements (`import logging`, `logger = getLogger`, `logger.*`) for `OpenAIProvider`.
    ai_assisted: true
    model: "gpt-4.1-mini"
    prompt_template: |
      Goal: Generate Python logging statements for `OpenAIProvider`.
      Context: Insert at class top and various method points.
    dependencies:
      - 3.4.3
    outputs:
      - "Logging snippets generated."

  - id: 3.5.2
    name: "[Optional] Review Logging Code Snippets"
    details: |
      Verify logging setup, levels, and message clarity.
    ai_assisted: false
    dependencies:
      - 3.5.1
    outputs:
      - "Logging snippets reviewed."

  - id: 3.5.3
    name: "[Optional] Integrate Logging into OpenAIProvider"
    details: |
      Insert reviewed logging statements into `openai_provider.py`.
    ai_assisted: false
    dependencies:
      - 3.4.3
      - 3.5.2
    outputs:
      - "Logging integrated in `openai_provider.py`."

  - id: 3.5.4
    name: "[Optional] Commit Logging Enhancement"
    details: |
      Stage and commit logging changes:
      Run:
        git add happy_llm_cli/providers/openai_provider.py
        git commit -m "feat(enhancement): Add logging to OpenAIProvider"
    ai_assisted: false
    dependencies:
      - 3.5.3
      - 3.4.4
    outputs:
      - "Logging enhancement committed."

fallbacks:
  - task_id: 3.x.1
    strategy: "Re-prompt or manually write the required code snippet."
  - task_id: 3.x.2
    strategy: "Correct the generated snippet manually before integration."
  - task_id: 3.x.3
    strategy: "Review insertion points and debug manually."
  - task_id: 3.1.4
    strategy: "Manually update dependencies and rebuild environment."

outputs:
  - "A functional `OpenAIProvider` class in `happy_llm_cli/providers/openai_provider.py`, incrementally implemented and committed."
  - "`complete_chat` handles request prep, API calls, response parsing, and error handling."
  - "(Optional) Logging integrated."
  - "`requests` dependency managed."
  - "Core OpenAI interaction functionality implemented and version controlled."